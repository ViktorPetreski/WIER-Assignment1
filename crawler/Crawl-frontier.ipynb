{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:34:35.631224Z",
     "start_time": "2020-04-03T11:34:35.405875Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from urllib.request import Request, urlopen\n",
    "from urllib.parse import urlparse\n",
    "import urllib.robotparser\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options as fOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import psycopg2\n",
    "import psycopg2.errorcodes as errorcodes\n",
    "import concurrent.futures\n",
    "import threading\n",
    "# new packages\n",
    "import mmh3\n",
    "from ordered_set import OrderedSet\n",
    "import pickle\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "warnings.filterwarnings('ignore')\n",
    "# http = urllib.PoolManager(num_pools=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:34:40.256178Z",
     "start_time": "2020-04-03T11:34:39.722459Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TIMEOUT = 5\n",
    "num_sites = 0\n",
    "numHashes = 25\n",
    "maxShingleID = 2**64-1\n",
    "nextPrime = 1.8446844e+19\n",
    "\n",
    "with open(\"coeffA.pickle\", \"rb\") as f:\n",
    "    coeffA = pickle.load(f)\n",
    "    \n",
    "with open(\"coeffB.pickle\", \"rb\") as f:\n",
    "    coeffB = pickle.load(f)\n",
    "    \n",
    "seed_sites_ids = {}\n",
    "with open(\"seed_sites_ids_latest.pickle\", \"rb\") as f:\n",
    "    seed_sites_ids = pickle.load(f)\n",
    "    \n",
    "with open(\"duplicate_urls_latest.pickle\", \"rb\") as f:\n",
    "    duplicate_urls = pickle.load(f)\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "seed_sites = set()\n",
    "with open(\"seed_sites_latest.pickle\", \"rb\") as f:\n",
    "    seed_sites.update(pickle.load(f))\n",
    "\n",
    "frontier = OrderedSet()\n",
    "\n",
    "with open(\"signatures_latest.pickle\", \"rb\") as f:\n",
    "    signatures = pickle.load(f)\n",
    "\n",
    "\n",
    "signatureForLink = {}\n",
    "with open(\"signatureForLink_latest.pickle\", \"rb\") as f:\n",
    "    signatureForLink = pickle.load(f)\n",
    "    \n",
    "robot_parser = {}\n",
    "with open(\"robot_parser_latest.pickle\", \"rb\") as f:\n",
    "    robot_parser = pickle.load(f)\n",
    "    \n",
    "duplicate_binaries = set()\n",
    "\n",
    "with open(\"duplicate_binaries_latest.pickle\", \"rb\") as f:\n",
    "    duplicate_binaries.update(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:44:37.856208Z",
     "start_time": "2020-04-03T11:44:37.846336Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "find_page_url_query= '''SELECT id, url FROM crawldb.page WHERE page_type_code = %s'''\n",
    "get_page_content= '''SELECT id, url, html_content FROM crawldb.page WHERE page_type_code = %s'''\n",
    "site_insert_query = '''INSERT INTO crawldb.site(domain, robots_content, sitemap_content) VALUES (%s,%s,%s) RETURNING id'''\n",
    "page_insert_query = '''INSERT INTO crawldb.page(site_id, page_type_code, url, html_content, http_status_code, accessed_time) VALUES (%s,%s,%s,%s,%s,%s) RETURNING id'''\n",
    "page_data_insert_query = ''' INSERT INTO crawldb.page_data(page_id, data_type_code, data) VALUES (%s,%s,%s)'''\n",
    "image_insert_query = ''' INSERT INTO crawldb.image(page_id, filename, content_type, accessed_time,data) VALUES (%s,%s,%s,%s,%s) RETURNING id'''\n",
    "link_insert_query = ''' INSERT INTO crawldb.link(from_page, to_page) VALUES (%s,%s)'''\n",
    "\n",
    "find_page_id_query = '''SELECT id FROM crawldb.page WHERE url = %s'''\n",
    "update_page_query = ''' UPDATE crawldb.page SET page_type_code=%s, html_content=%s, http_status_code=%s,accessed_time=%s WHERE id=%s'''\n",
    "update_page_content = ''' UPDATE crawldb.page SET page_type_code=%s, html_content=%s WHERE id=%s'''\n",
    "get_frontier_query = \"\"\"SELECT id, url FROM crawldb.page WHERE page_type_code='FRONTIER' and not (url like '%bold_mode%' or url like '%caps_mode%' or url like '%view_mode%' or url like '%login%' or url like '%jpg%' or url like '%auth%' or url like '%download%' or url like '%file%')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:42:20.049757Z",
     "start_time": "2020-04-03T11:42:20.026402Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open ('frontier_lodi.pickle', 'rb') as fp:\n",
    "    skip_frontier = pickle.load(fp)\n",
    "# TIMEOUT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:54:48.137581Z",
     "start_time": "2020-04-03T11:54:47.726597Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=\"localhost\", user=\"postgres\", password=\"postgres\")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(get_frontier_query)\n",
    "p = cursor.fetchall()\n",
    "frontier = OrderedSet()\n",
    "frontier.update(p)\n",
    "print(len(frontier))\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:45:10.553759Z",
     "start_time": "2020-04-03T11:45:10.538501Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For each of the 'numHashes' hash functions, generate a different coefficient 'a' and 'b'.   \n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    return tokens\n",
    "\n",
    "def generate_shingles(tokens):\n",
    "    shinglesInHTML = set()\n",
    "    for index in range(0, len(tokens) - 2):\n",
    "        shingle = \"{} {} {}\".format(tokens[index], tokens[index + 1], tokens[index + 2])\n",
    "        hashed_shingle = mmh3.hash64(shingle)[0] & 0xffffffffffffffff\n",
    "        shinglesInHTML.add(hashed_shingle)\n",
    "#     print(shinglesInHTML)\n",
    "    return shinglesInHTML\n",
    "\n",
    "def minhash(shingleIDSet):\n",
    "\n",
    "    # The resulting minhash signature for this document. \n",
    "    signature = []\n",
    "\n",
    "    # For each of the random hash functions...\n",
    "    for i in range(0, numHashes):\n",
    "\n",
    "        # For each of the shingles actually in the document, calculate its hash code\n",
    "        # using hash function 'i'. \n",
    "\n",
    "        # Track the lowest hash ID seen. Initialize 'minHashCode' to be greater than\n",
    "        # the maximum possible value output by the hash.\n",
    "        minHashCode = nextPrime + 1\n",
    "\n",
    "        # For each shingle in the document...\n",
    "        for shingleID in shingleIDSet:\n",
    "            # Evaluate the hash function.\n",
    "#             print(shingleID.bit_length())\n",
    "            hashCode = (coeffA[i] * shingleID + coeffB[i]) % nextPrime \n",
    "\n",
    "            # Track the lowest hash code seen.\n",
    "            if hashCode < minHashCode:\n",
    "                minHashCode = hashCode\n",
    "\n",
    "        # Add the smallest hash code value as component number 'i' of the signature.\n",
    "        signature.append(minHashCode)\n",
    "\n",
    "    # Store the MinHash signature for this document.\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:45:10.919520Z",
     "start_time": "2020-04-03T11:45:10.905500Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compare_signatures(signature):\n",
    "    t0 = time.time()\n",
    "    duplicate_signature = None\n",
    "    count = 0\n",
    "    # For each of the test documents...\n",
    "    for signature2 in signatures:\n",
    "        count = 0\n",
    "        for k in range(0, numHashes):\n",
    "            count += (signature[k] == signature2[k])\n",
    "        if count/numHashes > 0.8:\n",
    "            break\n",
    "    print(f\"HASH: {count/numHashes}\")\n",
    "    # Calculate the elapsed time (in seconds)\n",
    "    elapsed = (time.time() - t0)\n",
    "\n",
    "    print (\"Comparing MinHash signatures took %.2fsec\" % elapsed)\n",
    "    return signature2 if count/numHashes > 0.8 else None\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:55:15.393091Z",
     "start_time": "2020-04-03T11:55:15.161073Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binary_file_matcher = re.compile(\"(\\.doc|\\.docx|\\.xls|\\.xlsx|\\.pdf|\\.ppt|\\.pptx|\\.png|\\.jpg|\\.jpeg|\\.gif)$\")\n",
    "tmp_frontier = OrderedSet()\n",
    "for vv in frontier:\n",
    "    if vv[1] not in skip_frontier and not binary_file_matcher.search(vv[1]) and not (\"mailto:\" in vv[1] or \"zip\" in vv[1] or \".xls\" in vv[1] or \"pdf\" in vv[1]\\\n",
    "                or \".csv\" in vv[1] or \".odt\" in vv[1] or \".ods\" in vv[1] or \"jpg\" in vv[1]\\\n",
    "                or \"bold_mode\" in vv[1] or \"view_mode\" in vv[1] or \"file\" in vv[1]\\\n",
    "                or \"dokument\" in vv[1] or \"caps_mode\" in vv[1] or \"download\" in vv[1]\\\n",
    "                or \"login\" in vv[1] or \"auth\" in vv[1] or \"ppt\" in vv[1]\\\n",
    "                or \".crt\" in vv[1] or \".pem\" in vv[1] or \".crl\" in vv[1] ):\n",
    "        tmp_frontier.add(vv)\n",
    "frontier = OrderedSet() | tmp_frontier\n",
    "print(len(frontier))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signatures = []\n",
    "signatureForLink = {}\n",
    "# conn = psycopg2.connect(host=\"localhost\", user=\"postgres\", password=\"postgres\")\n",
    "# conn.autocommit = True\n",
    "# original_parent_id = None\n",
    "for content in page_content:\n",
    "    tokens = preprocess(content[2])\n",
    "    shingles_in_html = generate_shingles(tokens)\n",
    "    signature = minhash(shingles_in_html)\n",
    "    p = compare_signatures(signature)\n",
    "\n",
    "    signatures.append(signature)\n",
    "    signature_copy = signature.copy()\n",
    "    signature_copy.sort()\n",
    "    mapped_sorted_os = map(str, signature_copy)\n",
    "    string_sorted_os = \",\".join(mapped_sorted_os)\n",
    "    signatureForLink[string_sorted_os] = content[1] \n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:45:16.969356Z",
     "start_time": "2020-04-03T11:45:16.957243Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def driver_info(name):\n",
    "    if name == \"Lodi\":\n",
    "#         WEB_DRIVER_LOCATION = \"D:/FRI/Summer Semester 2020/Web information extraction and retrieval/Assignment 1/geckodriver-v0.26.0-win64/geckodriver.exe\"\n",
    "#         options = fOptions()\n",
    "#         driver = webdriver.Firefox(executable_path=WEB_DRIVER_LOCATION,options=options)\n",
    "        WEB_DRIVER_LOCATION = \"D:/FRI/Summer Semester 2020/Web information extraction and retrieval/Assignment 1/geckodriver-v0.26.0-win64/geckodriver.exe\"\n",
    "        options = fOptions()\n",
    "        # If you comment the following line, a browser will show ...\n",
    "        options.add_argument(\"--headless\")\n",
    "        #Adding a specific user agent\n",
    "        options.add_argument(\"user-agent=fri-ieps-26\")\n",
    "        driver = webdriver.Firefox(executable_path=WEB_DRIVER_LOCATION,options=options)        \n",
    "        \n",
    "        \n",
    "    elif name == \"Viktor\":\n",
    "        WEB_DRIVER_LOCATION = \"/home/viktor/Documents/FRI/WIER/Assignments/geckodriver\"\n",
    "        options = fOptions()\n",
    "        # If you comment the following line, a browser will show ...\n",
    "        options.add_argument(\"--headless\")\n",
    "        #Adding a specific user agent\n",
    "        options.add_argument(\"user-agent=fri-ieps-26\")\n",
    "        driver = webdriver.Firefox(executable_path=WEB_DRIVER_LOCATION,\n",
    "                               options=options)\n",
    "          \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T11:45:20.469319Z",
     "start_time": "2020-04-03T11:45:20.455054Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_response_code(url):\n",
    "    if \"mailto:\" in url:\n",
    "        print(\"skipping: e-mail\")\n",
    "        return 503, None, []\n",
    "    try:\n",
    "        response = requests.head(url, verify=False, allow_redirects=True, timeout=1) \n",
    "        return response.status_code, response.url, response.history\n",
    "    except Exception:\n",
    "        print(\"Timeout exception for url: \", url)\n",
    "        return 503, url, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T12:01:24.388616Z",
     "start_time": "2020-04-03T12:01:24.351131Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_content(page_id_parent, parent_url):\n",
    "    parent_url = parent_url.replace(\"www.\", \"\")\n",
    "    parsed_url = urlparse(parent_url)\n",
    "    rp = None\n",
    "    try:\n",
    "        rp = robot_parser[parsed_url.netloc]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if rp is None or not rp.can_fetch(\"fri-ieps-26\",parent_url):\n",
    "        return\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"postgres\", password=\"postgres\")\n",
    "    conn.autocommit = True\n",
    "    driver = driver_info(\"Viktor\")\n",
    "    accessed_time_parent_url = datetime.now()\n",
    "    print(f\"Retrieving web page URL '{parent_url}'\")\n",
    "    driver.get(parent_url)\n",
    "    \n",
    "     # Timeout needed for Web page to render (read more about it)\n",
    "    time.sleep(TIMEOUT)\n",
    "    images = driver.find_elements_by_tag_name(\"img\")\n",
    "    for image in images:\n",
    "        src = image.get_attribute(\"src\")\n",
    "        img_id = None\n",
    "        page_id = None\n",
    "        add_link_flag = False\n",
    "        if src:\n",
    "            gov_si_matcher = re.match(\"(http|https)://(.*)\\.?gov\\.si(.*)\",src)\n",
    "            if gov_si_matcher:\n",
    "                if src not in duplicate_binaries:\n",
    "                    image_sources.add(src)\n",
    "                    duplicate_binaries.add(src)\n",
    "                    accessed_time = datetime.now()\n",
    "                    http_status_code, http_redirect_url, res_history = get_response_code(url)\n",
    "                    time.sleep(TIMEOUT)\n",
    "\n",
    "                     # ADD CURRENT IMAGE TO PAGE DB\n",
    "                    try:\n",
    "                        with lock:\n",
    "                            cursor = conn.cursor()\n",
    "                            cursor.execute(page_insert_query, (site_id, \"BINARY\", src, html_content_binary, http_status_code, accessed_time))\n",
    "                            page_id = cursor.fetchone()[0]\n",
    "                            conn.commit()\n",
    "                            add_link_flag = True\n",
    "                            cursor.close()\n",
    "\n",
    "                    except (Exception, psycopg2.DatabaseError) as error :\n",
    "                        if error.pgcode == errorcodes.UNIQUE_VIOLATION or error.pgcode == \"23505\" or \"duplicate\" in error:\n",
    "                            pass\n",
    "                        else:\n",
    "                            print (\"Error while writing binary page\", src, \" to DB: \", error)\n",
    "                \n",
    "                if page_id is not None:\n",
    "\n",
    "                    # ADD CURRENT IMAGE TO IMAGE DB\n",
    "                    file_name = src.rsplit(\"/\", 1)[-1]\n",
    "                    filename= file_name.split('.')[0]\n",
    "                    content_type = file_name.split('.')[-1].lower()\n",
    "                    if content_type in img_types_dict:\n",
    "                        content_type = img_types_dict[content_type]\n",
    "                    else:\n",
    "                        content_type = None\n",
    "                    try:\n",
    "                        with lock:\n",
    "                            cursor = conn.cursor()\n",
    "                            cursor.execute(image_insert_query, (page_id, filename, content_type, accessed_time, html_content_binary))\n",
    "                            img_id = cursor.fetchone()[0]\n",
    "                            conn.commit()\n",
    "                            cursor.close()\n",
    "                            \n",
    "                    except (Exception, psycopg2.DatabaseError) as error :\n",
    "                        print (\"Error while writing image \", src, \" to DB: \", error)\n",
    "\n",
    "                    if add_link_flag and page_id_parent is not None:\n",
    "                    # ADD LINKS TO DB\n",
    "                        try:\n",
    "                            with lock:\n",
    "                                cursor = conn.cursor()\n",
    "                                cursor.execute(link_insert_query, (page_id_parent, page_id))\n",
    "                                conn.commit()\n",
    "                                cursor.close()\n",
    "                        except (Exception, psycopg2.DatabaseError) as error :\n",
    "                            print (\"Error while writing link from : \", parent_url, \" to \",src,\" :\", error)\n",
    "    html = driver.page_source\n",
    "    \n",
    "    # extract the text from the raw html (everything below)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    [\n",
    "        s.extract()\n",
    "        for s in soup(['style', 'script', '[document]', 'head', 'title'])\n",
    "    ]\n",
    "    text = soup.getText()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    content = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    tokens = preprocess(content)\n",
    "    shingles_in_html = generate_shingles(tokens)\n",
    "    signature = minhash(shingles_in_html)\n",
    "    original_signature = compare_signatures(signature)\n",
    "    code = \"HTML\"\n",
    "    original_parent_id = None\n",
    "    if original_signature is not None:\n",
    "        code = \"DUPLICATE\"\n",
    "        content = None\n",
    "        original_signature.sort()\n",
    "        mapped_sorted_os = map(str, original_signature)\n",
    "        string_sorted_os = \",\".join(mapped_sorted_os)\n",
    "        original_url = signatureForLink[string_sorted_os]\n",
    "        try:\n",
    "            with lock:\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(find_page_id_query,(original_url,))\n",
    "                original_parent_id = cursor.fetchone()[0]\n",
    "                cursor.close()\n",
    "        except (Exception, psycopg2.DatabaseError) as error :\n",
    "            print (\"Error while trying to fetch page id of: \", original_url, \": \", error)\n",
    "        try:\n",
    "            with lock:\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(link_insert_query, (original_parent_id, page_id_parent))\n",
    "                conn.commit()\n",
    "                cursor.close()\n",
    "        except (Exception, psycopg2.DatabaseError) as error :\n",
    "            print (\"Error while writing duplicate link from : \", original_parent_id, \" to \",page_id_parent,\" :\", error)\n",
    "    else:\n",
    "        signatures.append(signature)\n",
    "        signature_copy = signature.copy()\n",
    "        signature_copy.sort()\n",
    "        mapped_sorted_os = map(str, signature_copy)\n",
    "        string_sorted_os = \",\".join(mapped_sorted_os)\n",
    "        signatureForLink[string_sorted_os] = parent_url \n",
    "    \n",
    "    if page_id_parent is not None:\n",
    "        # The page is already in the DB. We need to update the information.\n",
    "        print(\"UPDATING PARENT: \", parent_url)\n",
    "        http_status_code, http_redirect_url, res_history = get_response_code(parent_url)\n",
    "        time.sleep(TIMEOUT)\n",
    "        try:\n",
    "            with lock:\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(update_page_query,(code,content,http_status_code,accessed_time_parent_url,page_id_parent))\n",
    "                cursor.close()\n",
    "                conn.commit()\n",
    "                \n",
    "        except (Exception, psycopg2.DatabaseError) as error :\n",
    "            print (\"ERROR WHILE UPDATING PAGE \", parent_url,\" : \", error)\n",
    "    driver.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T21:18:24.513388Z",
     "start_time": "2020-04-03T21:06:05.659961Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i <= len(frontier):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        print(f\"\\n ... executing workers ...\\n\")\n",
    "        for _ in range(20):\n",
    "            executor.submit(extract_content, frontier[i][0], frontier[i][1])\n",
    "            i += 1\n",
    "        print(f\"i: {i}, frontier: {len(frontier)}\")\n",
    "    clear_output(wait=True)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
